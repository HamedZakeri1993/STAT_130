{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da914c6",
   "metadata": {},
   "source": [
    "# STA130 TUT 01 (Sep06)<br><br> üèÉüèª‚Äç‚ôÄÔ∏è üèÉüèª <u> Hitting the ground running... <u><br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bd2d500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e8a353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8969b8",
   "metadata": {},
   "source": [
    "\n",
    "## üöß üèóÔ∏è (Using notebooks and ChatBots) Demo [45 minutes]  \n",
    "      \n",
    "#### 1. *[About 8 of the 45 minutes]* Demonstrate going to the course [Quercus homepage](https://q.utoronto.ca/courses/354091); accessing the [Course GitHub Repo](https://github.com/pointOfive/STA130_ChatGPT); opening new and uploaded notebooks on [UofT Jupyterhub](https://datatools.utoronto.ca) (classic jupyter notebook, or jupyterhub is fine, or students may use [google collab](https://colab.research.google.com/)); and using Jupyter notebooks as a \"`Python` calculator\" and editing [\"Markdown cells\"](https://www.markdownguide.org/cheat-sheet/)<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> This is all simple and pretty obvious intended to be that way so students get that they can do this on their own.\n",
    "\n",
    "</details>\n",
    "    \n",
    "#### 2. *[About 30 of the 45 minutes]* Demonstrate using [ChatGPT](https://chat.openai.com/) (or [Copilot](https://copilot.microsoft.com/) if conversation privacy is desired) to\n",
    "\n",
    "1. find an (a) \"amusing, funny, or otherwise interesting dataset\" which (b) has missing values and is (c) available online through a URL link to a csv file;\n",
    "2. load the data into the notebook with `pandas` and get \"missing data counts\" for the dataset;\n",
    "3. prompt the ChatBot to \"Please provide a summary of our interaction for submitting as part of the requirements of an assignment\"; and, to \"Please provide me with the final working verson of the code that we created\"<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> The intention here is to demonstrate that [at the current GPT4.0-ish level] the ChatBot (with probably about an 80% chance) **cannot fullfil all the requests of the inquiry (of (a) \"funny or amusing\" nature, (b) the presence of missingness, and (c) working url links)** *but will otherwise produce working code*<br><br>\n",
    "> \n",
    "> 1. ChatBots have a notoriously \"short term memory\"; so, be ready for them to \"forget\" specific details of your prompting requests \n",
    "> 2. ChatBots often cannot pivot away substantially from initial answers; so, be ready for your efforts at follow up and correction with the ChatBot to prove frustratingly futile (which, may in this case actually have a lot to do with the following fact, that...)\n",
    "> 3. ChatBots don't seem to be very aware of the contents of datasets that are avalable online (or even working url links where datasets are); so, ChatBot are not currently a substitue for exploring dataset repository such as [TidyTuesday](https://github.com/rfordatascience/tidytuesday) (or other data repositiory resources) and reviewing data yourself (although, ChatBot interactions can nonetheless be help with brainstorm dataset ideas and provide a way to \"search for content\", perhaps especially when referencing a specific website in the conversation)<br><br>\n",
    "> \n",
    "> Examples of this task going pretty well are available [here](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/COP/SLS/00006_copilot_funnyamusingNAdatasetV3.md), [here](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/COP/SLS/00007_copilot_funnyamusingNAdatasetV4.md), and [here](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/GPT/SLS/00001_gpt3p5_villagersdata.md); while, examples of this going poorly are available [here](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/COP/SLS/00002_copilot_funnyamusingNAdataset.md) and [here](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/GPT/SLS/00002_gpt3p5_funnyasusingNAdataset.md). Successes and failures are found within the Microsoft Copilot and ChatGPT ChatBots both, suggesting the quality of the results likely has to do more with \"randomness\" and perhaps the nature of the prompting and engagement as opposed to the actual ChatBot version being used...\n",
    "    \n",
    "</details>\n",
    "\n",
    "#### 3. *[About 7 of the 45 minutes]* Demonstrate saving your python jupyter notebook in your own account and \"repo\" on [github.com](https://github.com), and sharing (a) notebook links, (b), ChatBot transcript log links, (c) ChatBot summaries through a piazza post and a Quercus announcement (so students can use this later for their homework assignment if they wish)<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345f8f0",
   "metadata": {},
   "source": [
    "# First Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173695a3",
   "metadata": {},
   "source": [
    "Dog Names in NYC Dataset. Chat gave me this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b76e5851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://data.cityofnewyork.us/api/views/25th-nujf/rows.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d9420b",
   "metadata": {},
   "source": [
    "Lets look at the head of hour dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6628cc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year of Birth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Child's First Name</th>\n",
       "      <th>Count</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>GERALDINE</td>\n",
       "      <td>13</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>GIA</td>\n",
       "      <td>21</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>GIANNA</td>\n",
       "      <td>49</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>GISELLE</td>\n",
       "      <td>38</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>GRACE</td>\n",
       "      <td>36</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year of Birth  Gender Ethnicity Child's First Name  Count  Rank\n",
       "0           2011  FEMALE  HISPANIC          GERALDINE     13    75\n",
       "1           2011  FEMALE  HISPANIC                GIA     21    67\n",
       "2           2011  FEMALE  HISPANIC             GIANNA     49    42\n",
       "3           2011  FEMALE  HISPANIC            GISELLE     38    51\n",
       "4           2011  FEMALE  HISPANIC              GRACE     36    53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "258165dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  HISPANIC\n",
       "1                  HISPANIC\n",
       "2                  HISPANIC\n",
       "3                  HISPANIC\n",
       "4                  HISPANIC\n",
       "                ...        \n",
       "69209        BLACK NON HISP\n",
       "69210    WHITE NON HISPANIC\n",
       "69211    WHITE NON HISPANIC\n",
       "69212    WHITE NON HISPANIC\n",
       "69213              HISPANIC\n",
       "Name: Ethnicity, Length: 69214, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5cebd5",
   "metadata": {},
   "source": [
    "This looks fishy, this does not look like dog breeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c2f27",
   "metadata": {},
   "source": [
    "# Chat Made a Mistake\n",
    "## Actual dog names in NYC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf01214",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://data.cityofnewyork.us/api/views/xx67-kt59/rows.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "url = \"https://data.cityofnewyork.us/api/views/xx67-kt59/rows.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d07d4465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52/2940949307.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(url)\n"
     ]
    }
   ],
   "source": [
    "url = \"https://data.cityofnewyork.us/api/views/nu7n-tubp/rows.csv\"\n",
    "df = pd.read_csv(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e77f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnimalName</th>\n",
       "      <th>AnimalGender</th>\n",
       "      <th>AnimalBirthYear</th>\n",
       "      <th>BreedName</th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>LicenseIssuedDate</th>\n",
       "      <th>LicenseExpiredDate</th>\n",
       "      <th>Extract Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PAIGE</td>\n",
       "      <td>F</td>\n",
       "      <td>2014</td>\n",
       "      <td>American Pit Bull Mix / Pit Bull Mix</td>\n",
       "      <td>10035.0</td>\n",
       "      <td>09/12/2014</td>\n",
       "      <td>09/12/2017</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOGI</td>\n",
       "      <td>M</td>\n",
       "      <td>2010</td>\n",
       "      <td>Boxer</td>\n",
       "      <td>10465.0</td>\n",
       "      <td>09/12/2014</td>\n",
       "      <td>10/02/2017</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALI</td>\n",
       "      <td>M</td>\n",
       "      <td>2014</td>\n",
       "      <td>Basenji</td>\n",
       "      <td>10013.0</td>\n",
       "      <td>09/12/2014</td>\n",
       "      <td>09/12/2019</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QUEEN</td>\n",
       "      <td>F</td>\n",
       "      <td>2013</td>\n",
       "      <td>Akita Crossbreed</td>\n",
       "      <td>10013.0</td>\n",
       "      <td>09/12/2014</td>\n",
       "      <td>09/12/2017</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOLA</td>\n",
       "      <td>F</td>\n",
       "      <td>2009</td>\n",
       "      <td>Maltese</td>\n",
       "      <td>10028.0</td>\n",
       "      <td>09/12/2014</td>\n",
       "      <td>10/09/2017</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AnimalName AnimalGender AnimalBirthYear  \\\n",
       "0      PAIGE            F            2014   \n",
       "1       YOGI            M            2010   \n",
       "2        ALI            M            2014   \n",
       "3      QUEEN            F            2013   \n",
       "4       LOLA            F            2009   \n",
       "\n",
       "                              BreedName  ZipCode LicenseIssuedDate  \\\n",
       "0  American Pit Bull Mix / Pit Bull Mix  10035.0        09/12/2014   \n",
       "1                                 Boxer  10465.0        09/12/2014   \n",
       "2                               Basenji  10013.0        09/12/2014   \n",
       "3                      Akita Crossbreed  10013.0        09/12/2014   \n",
       "4                               Maltese  10028.0        09/12/2014   \n",
       "\n",
       "  LicenseExpiredDate  Extract Year  \n",
       "0         09/12/2017          2016  \n",
       "1         10/02/2017          2016  \n",
       "2         09/12/2019          2016  \n",
       "3         09/12/2017          2016  \n",
       "4         10/09/2017          2016  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0889bc4",
   "metadata": {},
   "source": [
    "## Missing Values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dea138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnimalName            1387\n",
      "AnimalGender            21\n",
      "AnimalBirthYear          0\n",
      "BreedName                0\n",
      "ZipCode                  9\n",
      "LicenseIssuedDate        0\n",
      "LicenseExpiredDate      82\n",
      "Extract Year             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e5503",
   "metadata": {},
   "source": [
    "it seems that there are missing values for animal gender and licence expried date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b05904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       AnimalName AnimalGender AnimalBirthYear  \\\n",
      "1622          NaN            M            2014   \n",
      "2051          NaN            F            2014   \n",
      "2843          NaN            M            2014   \n",
      "3192          NaN            M            2014   \n",
      "3703          NaN            F            2014   \n",
      "...           ...          ...             ...   \n",
      "616757       NONE            F            2023   \n",
      "616781        NaN            F            2019   \n",
      "616808        NaN            M            2023   \n",
      "616828        NaN            M            2022   \n",
      "616851        NaN            M            2021   \n",
      "\n",
      "                               BreedName  ZipCode LicenseIssuedDate  \\\n",
      "1622                   Poodle, Miniature  10014.0        01/02/2015   \n",
      "2051                           Chihuahua  11223.0        01/09/2015   \n",
      "2843                Schnauzer, Miniature  10032.0        01/20/2015   \n",
      "3192         Miniature American Shepherd  10016.0        01/25/2015   \n",
      "3703                         Poodle, Toy  10014.0        02/01/2015   \n",
      "...                                  ...      ...               ...   \n",
      "616757  Dachshund, Long Haired Miniature  11952.0        10/31/2023   \n",
      "616781              Jack Russell Terrier  11372.0        11/01/2023   \n",
      "616808                          Shih Tzu  10463.0        11/01/2023   \n",
      "616828           Bichon Frise Crossbreed  10037.0        11/01/2023   \n",
      "616851                 Yorkshire Terrier  10452.0        11/01/2023   \n",
      "\n",
      "       LicenseExpiredDate  Extract Year  \n",
      "1622           01/02/2016          2016  \n",
      "2051           01/09/2016          2016  \n",
      "2843           01/20/2016          2016  \n",
      "3192           01/25/2016          2016  \n",
      "3703           02/01/2016          2016  \n",
      "...                   ...           ...  \n",
      "616757                NaN          2023  \n",
      "616781         12/11/2024          2023  \n",
      "616808         11/01/2024          2023  \n",
      "616828         11/18/2024          2023  \n",
      "616851         03/22/2024          2023  \n",
      "\n",
      "[1498 rows x 8 columns]\n",
      "1498\n"
     ]
    }
   ],
   "source": [
    "# Display rows where any value is missing\n",
    "rows_with_missing_values = df[df.isnull().any(axis=1)]\n",
    "print(rows_with_missing_values)\n",
    "print(len(rows_with_missing_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dda4c5",
   "metadata": {},
   "source": [
    "# Chat history link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8743e7b9",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/8c621cb8-7249-4c02-b09f-9b59c5985b91"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a909d36",
   "metadata": {},
   "source": [
    "# Summary of the chat "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0efdd7",
   "metadata": {},
   "source": [
    "1. Dataset Request:\n",
    "You were looking for a fun and interesting dataset that contained missing values and could be easily downloaded and accessed in Python using a direct URL link to a CSV file. You also specified that you didn‚Äôt want to use datasets from Kaggle.\n",
    "2. Initial Suggestions:\n",
    "I first suggested several datasets (like the Titanic dataset, Air Quality dataset, and more). However, you didn‚Äôt find those datasets fun or exciting.\n",
    "3. Human Names vs. Dog Names Confusion:\n",
    "I provided a dataset link from NYC‚Äôs open data portal, describing it as containing dog names. However, you pointed out that the dataset had an Ethnicity column, which indicated that it was actually about human baby names, not dogs.\n",
    "I acknowledged the mistake and corrected the suggestion, providing a new, working link for the actual NYC Dog Names dataset.\n",
    "4. HTTP 404 Error:\n",
    "You encountered an HTTP Error 404: Not Found when trying to access the corrected dog names dataset. I recognized that the URL was invalid and provided a new working URL that would successfully load the dog names data.\n",
    "5. DtypeWarning Issue:\n",
    "After loading the correct dataset, you received a DtypeWarning, indicating that one of the columns had mixed data types (e.g., some rows might have strings, while others might have numbers in the same column).\n",
    "I explained that this warning occurs when pandas tries to infer data types for large datasets, which can cause issues when there are mixed types in a column.\n",
    "I then suggested three ways to handle this:\n",
    "Option 1: Setting low_memory=False, which would process the file in chunks and avoid the warning.\n",
    "Option 2: Manually specifying the data type of the column to ensure consistency.\n",
    "Option 3: Ignoring the warning using warnings.filterwarnings() if it doesn't affect your analysis.\n",
    "6. Checking for Missing Values:\n",
    "You asked how to check if the dataset had missing values. I provided multiple methods using pandas to check for missing data:\n",
    "Method 1: Using isnull().sum() to count missing values in each column.\n",
    "Method 2: Using isnull().any() to check whether any missing values exist in the columns.\n",
    "Method 3: Using isnull().sum().sum() to get the total count of missing values in the entire dataset.\n",
    "Method 4: Using df.info() to get a summary of non-null counts in each column.\n",
    "7. Displaying Rows with Missing Values:\n",
    "You wanted to see the actual rows that contain missing values. I provided the following code to filter the dataset and display only rows with missing data:\n",
    "python\n",
    "Copy code\n",
    "rows_with_missing_values = df[df.isnull().any(axis=1)]\n",
    "print(rows_with_missing_values)\n",
    "This code filters the DataFrame to display rows where at least one value is missing, allowing you to inspect the specific entries that have missing data.\n",
    "Would you like to explore any of these points further or need help with another task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd6107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc0b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "379538b1",
   "metadata": {},
   "source": [
    "## üí¨ üó£Ô∏è Communication [55 minutes]  \n",
    "     \n",
    "#### 1. *[About 15 of the 55 minutes]* Ice breakers  and introductions, in 8 groups of 3 or thereabouts...\n",
    "    \n",
    "1. Each person may bring two emojis to a desert island... reveal your emojis at the same time... for emojis selected more than once the group should select one additional emoji\n",
    "2. Where are you from, what do you think your major might be, and what's an \"interesting\" fact that you're willing to share about yourself?\n",
    "        \n",
    "#### 2. *[About 10 of the 55 minutes]* These are where all the bullet holes were in the planes that returned home so far after some missions in World War II\n",
    "    \n",
    "1. Where would you like to add armour to planes for future missions?\n",
    "2. Hint: there is a hypothetical dataset of the bullet holes on the planes that didn't return which is what we'd ideally compare against the dataset we observe...\n",
    "        \n",
    "![Classic image of survivorship bias of WW2 planes](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Survivorship-bias.svg/640px-Survivorship-bias.svg.png)\n",
    "           \n",
    "#### 3. *[About 10 of the 55 minutes]* Monte Hall problem: there is a gameshow with three doors, one of which has a prize, and you select one of the doors and the gameshow host reveals one of the other two unchosen doors which does not have the prize... would you like to change your guess to the other unchosen door?\n",
    "\n",
    "![](https://mathematicalmysteries.org/wp-content/uploads/2021/12/04615-0sxvwbnzvvnhuklug.png)<br>\n",
    "       \n",
    "#### 4. *[About 20 of the 60 minutes]* Discuss the experience of the groups for the WW2 planes and Monte Hall problems\n",
    "\n",
    "1. For each of these problems, have students vote on whether their groups (a) agreed on answers from the beginning, (b) agreed only after some discussion and convincing, or (c) retained somewhat divided in their opinions of the best way to proceed<br><br>\n",
    "    \n",
    "2. Briefely identify the correct answer from the answers the groups arrived at<br><br>\n",
    "    \n",
    "3. **[If time permits... otherwise this is something students could consider after TUT]** Prompt a [ChatGPT](https://chat.openai.com/) [or [Copilot](https://copilot.microsoft.com/)] ChatBot to introduce and explain \"survivorship bias\" using spotify songs as an example and see if students are able to generalize this idea for the WW2 planes problem and if they find it to be a convincing argument to understand the problem<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> This could be done like [this](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/COP/SLS/00009_copilot_survivorshipbias_spotify.md) or [this](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/GPT/SLS/00003_gpt3p5_spotify_Survivorship_Bias.md), or you could instead try to approach things more generally like [this](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/GPT/SLS/00004_gpt3p5_general_Survivorship_Bias.md)\n",
    "> \n",
    "> Two ends of the ChatBot prompting spectrum are\n",
    "> \n",
    "> 1. creating an extensive prompt exhuastively specifying the desired response results; or, \n",
    "> 2. iteratively clarifying the desired response results through interactive ChatBot dialogue<br><br>\n",
    "> \n",
    "> This is to some degree a matter of preference regarding the nature of ChatBot conversation sessions, but there it may also be a lever to influence the nature of the responses provided by the ChatBot \n",
    "</details>\n",
    "    \n",
    "4. **[If time permits... otherwise this is something students could consider after TUT]** Prompt a [ChatGPT](https://chat.openai.com/) [or [Copilot](https://copilot.microsoft.com/)] ChatBot to introduce and explain the Monte Hall problem and see if the students find it understandable and convincing<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> ChatBots fail to correctly analyze the Monte Hall problem when they're asked for a formal probabilistic argument...\n",
    ">\n",
    "> - [ChatGPT fails by wrongly calculating a probability of 1/2...](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/GPT/SLS/00005_gpt3p5_MonteHallWrong.md)\n",
    "> - [Copilot fares similarly poorly without substantial guidance...](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/COP/SLS/00010_copilot_montehallwrong.md)<br><br>\n",
    "> \n",
    "> *demonstrating (a) that there are clear limits to how deeply ChatBots actually \"reason\", and (b) that they are instead better understood as simply being information regurgitation machines, and (c) that this means  they can suffer from the \"garbage in, garbage out\" problem if the quality of the information their responses are based on are is poor and inaccurate (as is notoriously the case in the Monte Hall problem, for which many incorrect mathematical analyses have been \"published\" into the collection of human generated textual data on which ChatBots are based)*\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb9622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
